# Embedding

### 概念

Embedding是机器学习和自然语言处理（NLP）等领域中非常重要的概念。

它是一种把离散的数据（如文字、图像等）转换成连续向量的方法，让计算机能够理解这些符号的语义关系。

##### 1、通俗理解

比如说一组词：

> ["猫", "狗", "汽车", "香蕉"]

对于机器来说，只理解数字，不能理解这些自然语言。不知道猫和狗是相近的。

Embedding的作用是将这些词映射成向量（例如一个三维空间中）：

```css
猫 → [0.8, 0.1, 0.4]
狗 → [0.7, 0.2, 0.3]
汽车 → [0.1, 0.9, 0.8]
香蕉 → [0.2, 0.7, 0.1]
```

可以看到猫和狗这样语义更接近的向量会更接近。

##### 2、数据理解

> Embedding是一种向量表示，把高维、离散的对象（如单词、图像等）映射到低维的连续实数向量空间中

数学上可以写作：`f: X → Rn`

- f：是映射函数

- X：是原始空间（比如词表）

- Rn：是 n 维向量空间（即 embedding 空间）

### 余弦相似度比较Embedding

余弦相似度是用来比较两个向量（比如两个句子的embedding）“语义上有多相似”的最常用的方法。

余弦相似度的数学定义是：

$$
\mathrm{cosine\_similarity}(A,B)
= \frac{A \cdot B}{\|A\|\ × \|B\|}
$$

其中：

- A 和 B 是两个向量；

- A⋅B 表示**点积**；

- ||A|| 表示向量的**模（长度）**；

- 结果范围在 **[-1, 1]**。

结果：

- 值接近1，那么说明相似度高，也就是两个向量方向相同，说明相似度越高。

- 值接近-1，那么说明相似度完全相反，也就是两个向量方向相反，说明相似度相反。

- 值接近0，说明完全不相干，也就是两个向量垂直，说明不相干。

**缺点：**

- 通过向量余弦相似度计算出的值很容易出现误判，比如`我很喜欢你，但你不喜欢我`和`我很喜欢你，但你也喜欢我`两句的余弦相似度接近1，但两句换表达的意思却差距很大。

### N-Gram（N元语法）提取特征

N-Gram是指把一段文字按照连续N个词（或字）作为一个单元切分。Gram表示“连续”，N表示连续的个数（词或字）。其用途是对内容做特征提取。

**举例：**

`我爱自然语言处理`

| N   | 解释          | 结果                |
| --- | ----------- | ----------------- |
| 1   | Unigram（1元） | 我，爱，自然，语言，处理      |
| 2   | Bigram（2元）  | 我爱，爱自然，自然语言，语言处理  |
| 3   | Trigram（3元） | 我爱自然，爱自然语言，自然语言处理 |

**N-Gram在语言模型中的用途：**

主要用来统计某个词在N-1个词的条件下出现的概率，语言模型通过训练大量的数据，来统计某些词出现在某些词后面的概率，来辅助续写的功能。

### 基于内容的推荐系统

基于内容的推荐系统顾名思义就是根据所选的内容推荐相似度高的内容。优势在于：

- 依赖度低，不需要动态的用户行为，只要有内容就可以进行推荐。

##### 基于内容推荐的步骤

1、**物品表示**（ Item Representation），为每个item提取内容特征（Content Features）。

2、**特征学习**（Profile Learning），根据用户历史喜欢的item的内容特征来学习用户的喜好特征（User Profile）。

3、**生成推荐**，通过User Profile和候选的Item的特征，推荐相关性最大的Item。

##### 举例：为酒店建立内容推荐系统

有一张酒店集合的表格，通过选中的酒店推送相似度最高的Top10酒店。表格内容为：

| name                           | address                                         | desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------------------------ | ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Hilton Garden Seattle Downtown | 1821 Boren Avenue, Seattle Washington 98101 USA | "Located on the southern tip of Lake Union, the Hilton Garden Inn Seattle Downtown hotel is perfectly located for business and leisure. <br/>The neighborhood is home to numerous major international companies including Amazon, Google and the Bill & Melinda Gates Foundation. A wealth of eclectic restaurants and bars make this area of Seattle one of the most sought out by locals and visitors. Our proximity to Lake Union allows visitors to take in some of the Pacific Northwest's majestic scenery and enjoy outdoor activities like kayaking and sailing. over 2,000 sq. ft. of versatile space and a complimentary business center. State-of-the-art A/V technology and our helpful staff will guarantee your conference, cocktail reception or wedding is a success. Refresh in the sparkling saltwater pool, or energize with the latest equipment in the 24-hour fitness center. Tastefully decorated and flooded with natural light, our guest rooms and suites offer everything you need to relax and stay productive. Unwind in the bar, and enjoy American cuisine for breakfast, lunch and dinner in our restaurant. The 24-hour Pavilion Pantry? stocks a variety of snacks, drinks and sundries." |
| Sheraton Grand Seattle         | 1400 6th Avenue, Seattle, Washington 98101 USA  | Located in the city's vibrant core, the Sheraton Grand Seattle provides a gateway to the diverse sights and sounds of the Pacific Northwest. Step out of our front doors to find gourmet dining and bars, world-class shopping, exciting entertainment, and iconic local attractions including the Pike Place Market, Space Needle and Chihuly Garden & Glass Museum. As one of only seven Sheraton hotels in North America to earn the esteemed Grand designation, guests can book confidently knowing they?re receiving the highest benchmark on product and service offerings available. Experience our recently completed multimillion-dollar transformation featuring all new guest rooms, an expanded Sheraton Club Lounge, and modern meeting & event spaces. Gather in our stylish new lobby and enjoy our private art collection featuring local artists while enjoying your favorite beverage from Starbucks. The Sheraton Grand features several dining options including Loulay Kitchen & Bar by James Beard award winning chef Thierry Rautureau.                                                                                                                                                              |
| Crowne Plaza Seattle Downtown  | 1113 6th Ave, Seattle, WA 98101                 | "Located in the heart of downtown Seattle, the award-winning <br/>Crowne Plaza Hotel Seattle ? Downtown offers an exceptional blend of service, style and comfort. You?ll notice Cool, Comfortable and Unconventional touches that set us apart as soon as you step inside. Marvel at stunning views of the city lights while relaxing in our new Sleep Advantage? Beds. Enjoy complimentary wireless Internet throughout the hotel and amenities to help you relax like our Temple Spa? Sleep Tight Amenity kits featuring lavender spray and lotions to help you rejuvenate and unwind. Enjoy an invigorating workout at our 24-hour fitness center, get dining suggestions from our expert concierge or savor sumptuous cuisine at our Regatta Bar & Grille restaurant where you can enjoy Happy Hour in our lounge daily from 4pm - 7pm and monthly drink specials. Come and experience all that The Emerald City has to offer with us!"                                                                                                                                                                                                                                                                                |
| Kimpton Hotel Monaco Seattle   | 1101 4th Ave, Seattle, WA98101                  | "What?s near our hotel downtown Seattle location? The better <br/>question might be what?s not nearby. In addition to being one of the hotels near Pike Place Market, here?s just a small sampling of the rest. Columbia Center, whose Sky View Observatory on the 73rd floor is the tallest public viewing area west of the Mississippi Historic 5th Avenue Theatre, home to musical productions Seattle Central Library, an architectural marvel. Within half a mile: The must-see Pike Place Market, which houses the original Starbucks Pioneer Square, Seattle?s original downtown. Seattle Art Museum. Fantastic shopping, including the flagship Nordstrom, Nordstrom Rack, Macy?s, Columbia Sportswear, Louis Vuitton, Arcteryx, and oodles of independent boutiques. The Great Wheel. Washington State Convention Center. Within about a mile: The iconic Space Needle.  Bell Street Pier Cruise Terminal at Pier 66. Sports stadiums CenturyLink Field and Safeco Field, home to the Seattle Seahawks, Seattle Mariners, and Seattle Sounders."                                                                                                                                                                   |
| ...                            | ...                                             | ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |

##### 1、物品表示

**先对源数据进行探索 ：**

先对表格进行探索，通过`pd`读取文件数据：

```python
# 支持中文
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']  # 用来正常显示中文标签
df = pd.read_csv('Seattle_Hotels.csv', encoding="latin-1")
# 数据探索
print(df.head())
print('数据集中的酒店个数：', len(df))
```

执行结果：

```shell
➜  hotel_recommendation python hotel_re.py
                             name                                          address                                               desc
0  Hilton Garden Seattle Downtown  1821 Boren Avenue, Seattle Washington 98101 USA  Located on the southern tip of Lake Union, the...
1          Sheraton Grand Seattle   1400 6th Avenue, Seattle, Washington 98101 USA  Located in the city's vibrant core, the Sherat...
2   Crowne Plaza Seattle Downtown                  1113 6th Ave, Seattle, WA 98101  Located in the heart of downtown Seattle, the ...
3   Kimpton Hotel Monaco Seattle                    1101 4th Ave, Seattle, WA98101  What?s near our hotel downtown Seattle locatio...
4              The Westin Seattle   1900 5th Avenue, Seattle, Washington 98101 USA  Situated amid incredible shopping and iconic a...
数据集中的酒店个数： 152
```

执行结果可以看得出来，表格有3列，表头分别是`name`、`address`、`desc`。

**通过N元语法统计特征（人可以直观看得出来）：**

对描述这一列进行N-Gram特征统计，这里选择3元，也就是连续3个词作为一个单元。

```python
# 创建英文停用词列表，统计时会将这些常用词排除掉
ENGLISH_STOPWORDS = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 
    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', 
    "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 
    'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 
    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 
    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 
    'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 
    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 
    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 
    "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', 
    "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', 
    "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 
    'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"
}

# 得到酒店描述中n-gram特征中的TopK个
def get_top_n_words(corpus, n=1, k=None):
    # 统计ngram词频矩阵，使用自定义停用词列表
    vec = CountVectorizer(ngram_range=(n, n), stop_words=list(ENGLISH_STOPWORDS)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    # 按照词频从大到小排序
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:k]
common_words = get_top_n_words(df['desc'], 3, 20)
```

执行结果：

```shell
➜  hotel_recommendation python hotel_re.py
```[('pike place market', 85), ('seattle tacoma international', 21), ('tacoma international airport', 21), ('free wi fi', 19), ('washington state convention', 17), ('seattle art museum', 16), ('place market seattle', 16), ('state convention center', 15), ('within walking distance', 14), ('high speed internet', 14), ('space needle pike', 12), ('needle pike place', 11), ('south lake union', 11), ('downtown seattle hotel', 10), ('sea tac airport', 10), ('home away home', 9), ('heart downtown seattle', 8), ('link light rail', 8), ('free high speed', 8), ('24 hour fitness', 7)]
```

##### 2、特征学习

将选中的酒店的desc和每个酒店的desc做相似度矩阵，可以通过余弦相似度来比较。

例子中使用TF-IDF来提取特征矩阵，并通过余弦相似度来计算相似度

```python
# 文本预处理
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
# 使用自定义的英文停用词列表替代nltk的stopwords
STOPWORDS = ENGLISH_STOPWORDS
# 对文本进行清洗
def clean_text(text):
    # 全部小写
    text = text.lower()
    # 用空格替代一些特殊符号，如标点
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    # 移除BAD_SYMBOLS_RE
    text = BAD_SYMBOLS_RE.sub('', text)
    # 从文本中去掉停用词
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    return text
# 对desc字段进行清理，apply针对某列
df['desc_clean'] = df['desc'].apply(clean_text)
# print(df['desc_clean'])
# exit(0)

# 建模
df.set_index('name', inplace = True)
# 使用TF-IDF提取文本特征，使用自定义停用词列表
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words=list(ENGLISH_STOPWORDS))
# 针对desc_clean提取tfidf
tfidf_matrix = tf.fit_transform(df['desc_clean'])
print('TFIDF feature names:')
#print(tf.get_feature_names_out())
print(len(tf.get_feature_names_out()))
# 计算酒店之间的余弦相似度（线性核函数）
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)
print(cosine_similarities)
print(cosine_similarities.shape)
indices = pd.Series(df.index) #df.index是酒店名称
```

运行结果：

```shell
➜  hotel_recommendation python hotel_re.py
TFIDF feature names:
3347
[[1.         0.03930478 0.09970189 ... 0.04875507 0.02296214 0.0262131 ]
 [0.03930478 1.         0.05892777 ... 0.06130707 0.01516305 0.03543109]
 [0.09970189 0.05892777 1.         ... 0.08718294 0.04218405 0.04985284]
 ...
 [0.04875507 0.06130707 0.08718294 ... 1.         0.05751589 0.03948025]
 [0.02296214 0.01516305 0.04218405 ... 0.05751589 1.         0.01602277]
 [0.0262131  0.03543109 0.04985284 ... 0.03948025 0.01602277 1.        ]]
(152, 152)
```

从结果可以看得出来，通过TF-IDF提取的特征数量一共有3347个（也就是3347个维度），并形成了一个[152,152]的相似度矩阵，从数据中可以看出，对角线上都是1，是因为酒店自己和自己是相同的，所以是1。

##### 3、生成推荐

得到了相似度矩阵，那么就可以按照相似度从高到低（排除自己）取10条，则是推荐Top10。

```python
# 基于相似度矩阵和指定的酒店name，推荐TOP10酒店
def recommendations(name, cosine_similarities = cosine_similarities):
    recommended_hotels = []
    # 找到想要查询酒店名称的idx
    idx = indices[indices == name].index[0]
    print('idx=', idx)
    # 对于idx酒店的余弦相似度向量按照从大到小进行排序
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)
    # 取相似度最大的前10个（除了自己以外）
    top_10_indexes = list(score_series.iloc[1:11].index)
    # 放到推荐列表中
    for i in top_10_indexes:
        recommended_hotels.append(list(df.index)[i])
    return recommended_hotels
print(recommendations('Hilton Seattle Airport & Conference Center'))
```

运行结果：

```shell
➜  hotel_recommendation python hotel_re.py
idx= 49
['Embassy Suites by Hilton Seattle Tacoma International Airport', 'DoubleTree by Hilton Hotel Seattle Airport', 'Seattle Airport Marriott', 'Four Points by Sheraton Downtown Seattle Center', 'Motel 6 Seattle Sea-Tac Airport South', 'Hampton Inn Seattle/Southcenter', 'Radisson Hotel Seattle Airport', 'Knights Inn Tukwila', 'Hotel Hotel', 'Home2 Suites by Hilton Seattle Airport']
```

### Word Embedding

通过上面的例子，我们能发现仅仅152家酒店的`desc`通过N-Gram+TF-IDF就提取出了3347个特征，这样的特征矩阵会变得非常大，计算量非常大。也会导致很多特征为0。这也就是**one-hot编码：**

```css
# 比如：苹果
[0, 0, 1, 0, 0 .... 0]
# 香蕉
[0, 0, 0, 1, 0 .... 0]
# 汽车
[1, 0, 0, 0, 0 .... 0]

# 维度非常大，而且特征为0的特别多，导致很多浪费
# 苹果和香蕉的距离与苹果和汽车一样远，导致向量无法计算语义是否相近
```

为了解决one-hot编码的问题，从而引入**Word Embedding**，将高维进行压缩到低维，让空间更加紧凑，让相似词在向量空间中彼此接近，从而能够通过向量表达词的“意义”。

##### Word Embedding的关键特征

**1.连续的、稠密的向量：**

不想one-hot非常稀疏，Embedding通常十几到几百维，大型模型都是1000+维。

**2.捕捉语义关系：**

向量接近 -> 相似词。

**3.可以做向量运算：**

```css
国王−男人+女人=王后
```

因为Embedding学到了语义，可以参与运算。

##### Word Embedding类型

**1.Word2Vec**

`Word2Vec`是Google提出的，生成Word Embedding的技术。

训练方法：

```css
1. 词 -> One-hot编码
2. One-hot + （训练出来的矩阵） -> 得到dense vector（即World Embedding）
3. 通过大量 “预测上下文” 的训练，让embedding不断调整
4. 最终得到每个词的一个向量（如维度 100、300）
```

##### 使用Word2Vec工具提取Word Embedding的例子

**任务要求：**

数据集：西游记

- 数据在journey_to_the_west.txt文档中

- 要求计算小说中的人物相似度，比如孙悟空与猪八戒、孙悟空与孙行者等。

**步骤：**

1. 使用分词工具进行分词，比如`NLTK`、`JIEBA`

2. 将训练语料转化成一个sentence的迭代器

3. 使用word2vec进行训练

4. 计算两个词的相似度

**1、分词**

使用JIEBA对资料库进行分词

```python
# 对txt文本进行中文分词
import os
import jieba
from utils import files_processing

# 对source_folder中的所有文件进行分词，结果保存到segment_folder中
source_folder = './journey_to_the_west/source'
segment_folder = './journey_to_the_west/segment'

# 对file_list中的所有文件进行分词，结果保存到segment_out_dir中
def segment_lines(file_list, segment_out_dir, stopwords=[]):
    # 读取source_floder中的所有文件
    for i, file in enumerate(file_list):
        # 定义分割后的文件名
        segment_out_name = os.path.join(segment_out_dir, 'segment_{}.txt'.format(i))
        # 打开文件并读取内容进行分词
        with open(file, 'rb') as f:
            document = f.read()
            document_cut = jieba.cut(document)
            sentence_segment = []
            for word in document_cut:
                if word not in stopwords:
                    sentence_segment.append(word)
            result = ' '.join(sentence_segment)
            result = result.encode('utf-8')
            with open(segment_out_name, 'wb') as f2:
                f2.write(result)

# 对source中的txt文件进行分词，输出到segment目录中
file_list=files_processing.get_files_list(source_folder, postfix='*.txt')
segment_lines(file_list, segment_folder)
```

**2、训练并计算相似度**

先将分词的结果集转换成一个sentence迭代器中，然后进行训练

```python
# 将Word转换成Vec，然后计算相似度
from gensim.models import word2vec
import multiprocessing

segment_folder = './journey_to_the_west/segment'

# 切分之后的句子合集
sentences = word2vec.PathLineSentences(segment_folder)

# 设置模型参数，进行训练
# vector_size = 是训练的向量维度
# window = 句子中当前单词和被预测单词的最大距离
# min_count = 需要训练词语的最小出现次数，默认为5
# worker,训练使用的线程数，默认为1即不使用多线程
model = word2vec.Word2Vec(sentences, vector_size=100, window=3, min_count=1)
print(model.wv['孙悟空']) # 输出孙悟空的向量表示
print(model.wv.similarity('孙悟空', '猪八戒'))
print(model.wv.similarity('孙悟空', '孙行者'))
# 孙悟空+唐僧-孙行者
print(model.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))

# 调整模型参数查看不同参数会有什么影响
model = word2vec.Word2Vec(sentences, vector_size=128, window=5, min_count=1, workers=multiprocessing.cpu_count())
print(model.wv.similarity('孙悟空', '猪八戒'))
print(model.wv.similarity('孙悟空', '孙行者'))
print(model.wv.most_similar(positive=['孙悟空', '唐僧'], negative=['孙行者']))
```

执行结果：

```shell
➜  word2vec python word_similarity.py
[-0.4441607   0.08694004  0.14212838 -0.02735835 -0.29131022 -0.7236214
  0.69345033  0.7677849  -0.6770862  -0.53996867 -0.0783458  -0.16486761
 -0.1272611   0.3137007  -0.15001774 -0.5579493  -0.01928517 -0.08579294
 -0.49418178 -0.6278328  -0.08849018  0.017534    0.17639618 -0.18942505
 -0.4950738  -0.3730523  -0.26951784  0.175507   -0.28704065 -0.6630959
  0.60986346 -0.35869062  0.25550926 -0.3678874  -0.1824098   0.408438
  0.7302974  -0.24360025 -0.220865   -0.21571863 -0.11483292 -0.19990537
 -0.41918483 -0.11747844  0.35385215 -0.23502092 -0.08156584 -0.04506612
  0.25916317  0.13113895  0.09829718 -0.17245498 -0.26020327 -0.27045977
 -0.01874874  0.29532272  0.2711974  -0.44603598 -0.20456268  0.26297158
  0.28102073 -0.14642581  0.53569216 -0.37607098 -0.60756135  0.51389563
  0.00611685  0.5053671  -0.5377575   0.32237715  0.24628448  0.49025756
  0.17246848  0.30587453  0.47478774  0.03502865  0.01427634  0.34325477
 -0.62773746 -0.08424307 -0.2405315   0.17561823 -0.26989686  0.724762
 -0.16670331 -0.3507071   0.1085161   0.23497587 -0.05010765  0.17610478
  0.17490065 -0.00666772  0.6802375  -0.3587926   1.0929455   0.57851493
 -0.04973654 -0.41715056  0.42544395  0.0769178 ]
0.97390574
0.984952
[('老', 0.984952986240387), ('大王', 0.9834406971931458), ('长老', 0.9816674590110779), ('菩萨', 0.9798691868782043), ('师兄', 0.9778801202774048), ('悟空', 0.9763389229774475), ('害怕', 0.9753580689430237), ('兄弟', 0.9750009179115295), ('陛下', 0.9749753475189209), ('们', 0.9745582342147827)]
0.9617496
0.9766326
[('大王', 0.9886555075645447), ('师兄', 0.9861342906951904), ('兄弟', 0.9822982549667358), ('老', 0.9817707538604736), ('长老', 0.9798793196678162), ('菩萨', 0.978074848651886), ('大哥', 0.9767054319381714), ('不消', 0.9758414030075073), ('陛下', 0.9741215705871582), ('悟空', 0.9720991849899292)]
```

从结果中可以大概看得出来，每个词都被转换成了一个100维和128维的向量。

且可以判断出孙悟空和孙行者更相似。

##### 在大模型中的运用

主要用来分析语义、语义理解模型（嵌入模型、Embedding模型），帮你找到相似片段。

Embedding排行榜：[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)

# 向量数据库

### 什么是向量数据库

用来专门存储和检索高维向量数据的数据库，它将数据（文本、图片、视频等）通过嵌入模型转换为向量形式，并通过高效的索引和搜索算法实现快速检索。

向量数据库的核心作用是实现相似性搜索，即通过计算向量之间的距离（余弦相似度、欧几里得距离等）来找到与目标向量最相似的其他向量。

它特别合适处理非结构化数据（如RAG）、支持语义搜索、内容推荐等场景。

##### 向量数据库如何工作的？

**存储：**

向量数据库通过将嵌入向量（Embedding）存储为高维空间中的点，并为每个向量分配唯一的ID。同时支持存储元数据。

**检索：**

通过近似最近邻（ANN）算法（如PQ等）对向量进行索引和快速检索。比如FAISS、Milvus等数据库通过高效的索引结构加速检索。

### 常见的向量数据库产品

##### 1、FAISS

由Facebook开发，专注于高性能的相似性搜索，适合大规模静态数据集。

**优势：** 检索速度快，支持多种索引类型。

**局限：** 主要用于静态数据，更新和删除操作较复杂。

##### 2、Milvus

开源，支持分布式架构和动态数据更新。

**优势：** 具备强大的扩展性和灵活的数据管理功能。

##### 3、Pinecone

托管的云原生向量数据库，支持高性能的向量检索

**优势：** 完全托管、易于部署，合适大规模生产环境。

### 向量数据库和传统关系型数据库

##### 1、数据类型

**关系型数据库：** 存储结构化数据（如表格、行、列）。

**向量数据库：** 存储高维向量数据，适合非结构化数据。

##### 2、查询方式

**关系型数据库：** 依赖精确匹配（如=、<、>）。

**向量数据库：** 基于相似度或距离度量（如欧几里得举例、余弦相似度等）

##### 3、应用场景

**关系型数据库：** 合适需要支持事务、结构化信息管理。

**向量数据库：** 适合语义搜索、内容推荐等需要相似性计算的场景。




