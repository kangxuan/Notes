# RAG

### 概念

Retrieval Augmented Generation，检索增强生成，是一种结合信息检索和文本生成的技术。RAG通过实时检索相关文档或信息，并将其作为上下文输入到生成模型中，从而提高生成结果的时效性和准确性。

![rag流程图](./images/rag流程.png)

### RAG的核心用途

**1、解决知识时效性问题**

LLM的训练数据通常是静态的，无法涵盖最新的信息，而RAG可以检索外部知识库实时更新信息。

**2、解决幻觉问题**

通过引入外部知识库，RAG能够减少模型生成虚假或不准确的胡编乱造。

**3、提升专业领域回答质量**

RAG能够结合垂直领域的专业知识库，生成更专业更有深度的回答。

### RAG的核心原理和流程

这里说的是NativeRAG，即推理中使用RAG。

**1、数据预处理**

- 知识库构建：收集并整理文档、网页、数据库等多源数据，构建知识库，一般由数据治理团队去做。

- 文档分块：将文档切分为合适大小的块（chunks），以便后续检索。

- 向量化存储：使用嵌入模型（如BGE、M3E等Embedding模型）将chunk换换为统一维度的向量，并存储到向量数据库中

**2、检索阶段**

- 查询处理：将用户的问题（query）转换成向量，并在向量数据库中进行相似度检索，并返回最相似的（最相关的）chunk。

- 重排序：对检索返回的数据进行相关性排序，选择最相关的片段作为生成阶段的输入和上下文。

**3、生成阶段**

- 上下文组装：将用户的问题（query）和检索到的chunk结合，形成增强的上下文输入

- 生成回答：LLM基于增强的上下文生成回答。

![RAG步骤](./images/rag步骤.png)

### 嵌入模型（Embedding模型）选择和分类

Embedding模型排名在huggingface网站中实时更新，可根据实际情况选择：[Embedding排名网站]([MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard))

**1、通用文本嵌入模型**

- BGE-M3（智源研究院）：支持100+语言，融合密集、稀疏、多向量混合检索，适合跨语言长文档检索。适合跨语言长文档检索、高精度RAG应用。

- text-embedding-3-large（OpenAI）：向量维度3072，长文本语义捕捉能力强，英文表现优秀。英文内容优先的全球化应用。

**2、中文嵌入模型**

**3、指令驱动和复杂任务模型**

**4、企业级与复杂系统**

### CASE1：BGE-M3使用

针对于BGE-M3模型的使用，需要运行在GPU上，可以在AutoDL上租用GPU服务器试用。

**1、先下载必要的包**

```shell
pip install modelscope
pip install FlagEmbedding
```

**2、下载bge-m3模型** 

```python
#模型下载
from modelscope import snapshot_download
model_dir = snapshot_download('BAAI/bge-m3', cache_dir='/root/autodl-tmp/models')
```

**3、使用bge-m3计算相似度**

```python
from FlagEmbedding import BGEM3FlagModel

# 声明Embedding模型
model = BGEM3FlagModel('/root/autodl-tmp/models/BAAI/bge-m3',  
                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation
#
sentences_1 = ["What is BGE M3?", "Defination of BM25"]
sentences_2 = ["BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.", 
               "BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document"]

# 将文本转换为向量
embeddings_1 = model.encode(sentences_1, 
                            batch_size=12, 
                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.
                            )['dense_vecs']
# 将文本转化为向量
embeddings_2 = model.encode(sentences_2)['dense_vecs']
# 计算两组Embedding的相似度矩阵，@ 符号在Python中表示矩阵乘法运算
similarity = embeddings_1 @ embeddings_2.T
print(similarity)
```

运行结果：

```shell
[[0.626 0.3477]
[0.3499 0.678 ]]
```

从输入结果可以看出sentences_1中的第一个元素和sentences_2中的第一个元素更相似，sentences_1中的第二个元素和sentences_2中的第二个元素更相似。

### CASE2：gte-qwen2使用

**1、下载必要的包**

```shell
pip install modelscope
pip install FlagEmbedding
```

**2、下载gte-qwen2模型**

get-qwen2-7B模型太大，这里测试使用1.5B

```python
from modelscope import snapshot_download
#model_dir = snapshot_download('iic/gte_Qwen2-7B-instruct', cache_dir='/root/autodl-tmp/models')
model_dir = snapshot_download('iic/gte_Qwen2-1.5B-instruct', cache_dir='/root/autodl-tmp/models')
```

**3、使用gte-qwen2计算相似度**

```python
from sentence_transformers import SentenceTransformer

model_dir = "/root/autodl-tmp/models/iic/gte_Qwen2-1___5B-instruct"
model = SentenceTransformer(model_dir, trust_remote_code=True)
# 设置向量维度
model.max_seq_length = 8192

queries = [
    "how much protein should a female eat",
    "summit define",
]
documents = [
    "As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",
    "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.",
]

# 将文本转化为向量
query_embeddings = model.encode(queries, prompt_name="query")
document_embeddings = model.encode(documents)

# 计算相似度矩阵
scores = (query_embeddings @ document_embeddings.T) * 100
print(scores.tolist())
```

运行结果：

```shell
[[70.00668334960938, 8.184843063354492], [14.62419319152832, 77.71407318115234]]
```

和BGE-M3效果差不多，总体是可以看出谁和谁更相似。

### CASE3：Deepseek+Faiss搭建本地知识库检索

##### 需求

提供了一份名为`浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf`的文件，要求通过搭建本地知识库，结合LLM实现内部知识点问答。

##### 实现流程：

**1、从PDF中提取文本**

将提供的PDF文档通过`PdfReader`工具读取文本

**2、分块存入向量数据库中**

将文本按照chunk_size=800,chunk_overlap=150（overlap是前后的冗余量）进行切分，然后将切分好的chunk通过`text-embedding-v1`嵌入模型转换成同维度的向量，然后将chunk和转换后的向量存储到Faiss本地向量数据库中

**3、生成回答**

用户提问（query）从向量数据库中检索到相似的chunk，组合成增强的prompt向LLM提问，返回增强的回答。

```python
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
import os, pickle
from typing import List

DASHSCOPE_API_KEY = "sk-xxxxxxx"

# ======================
# 🧩 PDF 文本提取 + 页码
# ======================
def extract_text_with_page_numbers(pdf) -> (str, List[int]):
    text = ""
    page_numbers = []

    for page_number, page in enumerate(pdf.pages, start=1):
        extracted = page.extract_text()
        if extracted:
            lines = extracted.split("\n")
            text += extracted + "\n"
            page_numbers.extend([page_number] * len(lines))
        else:
            print(f"⚠️ Page {page_number} 无文本")

    return text, page_numbers


# ======================
# 🧩 向量存储
# ======================
def build_vector_db(text: str, page_numbers: List[int], save_path="./vector_db"):

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
    )
    chunks = splitter.split_text(text)
    print(f"文本分成 {len(chunks)} 块")

    embeddings = DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=DASHSCOPE_API_KEY
    )

    db = FAISS.from_texts(chunks, embeddings)

    # 建立文本→页码映射
    db.page_info = {chunk: page_numbers[i] for i, chunk in enumerate(chunks)}

    os.makedirs(save_path, exist_ok=True)
    db.save_local(save_path)

    with open(os.path.join(save_path, "page_info.pkl"), "wb") as f:
        pickle.dump(db.page_info, f)

    print("向量库存储完成")
    return db

# 加载向量库
def load_vector_db(path="./vector_db"):
    embeddings = DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=DASHSCOPE_API_KEY
    )
    db = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

    with open(os.path.join(path, "page_info.pkl"), "rb") as f:
        db.page_info = pickle.load(f)

    print("向量库加载成功")
    return db


# ======================
# 🧩 LLM + 检索 + QA 组合链（LCEL核心）
# ======================
llm = Tongyi(model_name="deepseek-v3", dashscope_api_key=DASHSCOPE_API_KEY)

prompt = PromptTemplate.from_template(
    """使用以下内容回答最终问题：
内容：{context}
问题：{question}
答案："""
)

def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

def ask(knowledgeBase, query):
    # 从向量数据库中召回多少个chunk（k=5，是召回5个chunk）
    # 通过相似语义改写as_retriever
    retriever = knowledgeBase.as_retriever(k=5)

    chain = (
        RunnableParallel({
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
        })
        | prompt
        | llm
    )

    result = chain.invoke(query)
    print("\n🧠 回答：\n", result)

    print("\n📌 来源页码：")
    docs = knowledgeBase.similarity_search(query, k=5)
    pages = set()
    for d in docs:
        page = knowledgeBase.page_info.get(d.page_content, "未知")
        if page not in pages:
            pages.add(page)
            print(f"- 第 {page} 页")


# ======================
# 🚀 主流程
# ======================
pdf_reader = PdfReader("./浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf")

text, page_numbers = extract_text_with_page_numbers(pdf_reader)

knowledgeBase = build_vector_db(text, page_numbers)

ask(knowledgeBase, "客户经理每年评聘申报时间是怎样的？")
```

运行结果：

```shell
文本分成 6 块
向量库存储完成

🧠 回答：
 客户经理每年评聘申报时间是**每年一月份**。

依据文件第六章第十一条规定：“每年一月份为客户经理评聘的申报时间，由分行人力资源部、个人业务部每年二月份组织统一的资格考试。”

📌 来源页码：
- 第 1 页
```

### LangChain问答链

LangChain是一个专门为构建基于大语言模型（LLM）的应用而设计的框架，由python语言开发。可以理解成大语言模型应用开发工具箱，里面集成了很多的工具。

LangChain问答链（Question-Answering Chain）是LangChain工具箱中的工具。它是一个预定义的流程：

> 输入用户问题 → (可选)查找相关文本 → 利用LLM生成回答

LangChain问答链有很多的模式，如下：

##### 1、stuff

直接把文档作为prompt输入给OpenAI。

##### 2、map_reduce

对于每个chunk做一个prompt（回答或者摘要），然后再做合并。

##### 3、refine

在第一个chunk上做prompt得到结果，然后合并下一个文件再输出结果。

##### 4、map_rerank

对每个chunk做prompt，然后打个分，然后根据分数返回最好的文档中的结果。

### RAG在LLM各个阶段的应用

##### 1、NativeRAG

原生RAG，不依赖LangChain框架，以更加底层的方式实现检索增强回答。

> 用户提问 → 检索向量数据库 → 将用户提问和检索到的chunks拼装成prompt → 发送给LLM → 输出回答。

**依赖的技术栈：**

- 文本提取工具：将数据提取出来，如PyPDF2。

- 数据切分并转成向量：Embedding模型。

- 存入本地向量数据库：Faiss等。

- 调用LLM。

上面讲解的RAG就是NativeRAG。

##### 2、RAFT

`Retrieval-Augmented Fine-Tuning`，即检索增强微调。它是RAG的进阶版本，让模型在微调过程中学习知识库的表达和提问方式。

相比RAG来说，RAG是将检索到的知识当做prompt给LLM，相当于外挂数据库。

而RAFT是在模型训练阶段中实现的，构建带检索上下文的训练样本，然后进行微调得到更强的LLM模型。

> 构建带检索上下文的训练样本 → 微调LLM（SFT-监督学习）→ 得到更强的LLM → 仍然可以外挂RAG

RAFT 是将检索增强的能力通过微调“融入模型本身”的技术，是RAG的高阶形态。

**RAG和RAFT对比：**

| 特性       | RAG   | RAFT    | RAG + RAFT |
| -------- | ----- | ------- | ---------- |
| 是否需要微调模型 | ❌ 不需要 | ✅ 需要    | 可选         |
| 知识更新速度   | 快     | 慢（重新训练） | 快          |
| 表达一致性    | 一般    | 强       | 很强         |
| 领域专业性    | 中     | 强       | 最强         |

# RAG高效召回方法

前面我们知道了RAG的召回是根据向量相似度进行召回，但是有时候向量相似度召回的并不一定最好的。

比如说提问：张三的女儿年龄是多少？现在有两个chunk，分别是：

```css
# chunk1
张三的女儿叫AAA
# chunk2
AAA今年14岁
```

这样通过相似度召回可能会找回chunk1，就可能导致回答效果不好了。所以我们需要采取一些方法提高RAG的召回效率。

### 1、改进检索算法

##### 1.1、知识图谱

 利用知识图谱中的语义信息和实体关系，增强对查询和文档的理解，提升召回的相关性。比如GraphRAG。

**CASE1：GraphRAG使用**

GraphRAG是一种结构化、分层的检索生成（RAG）方法，而不是使用纯文本片段的语义检索方法。即基于图谱结构进行推理，从而实现更聪明的检索。

> 原始文档中提取知识图谱 → 构建社区层级（个体之间的关系社区）→ 为这些层级生成摘要

### 2、引入重排序（Reranking）

##### 2.1、重排序模型

对召回的chunk进行重新排序，提升问题和文档的相关性。常见的重排序模型有`BGE-Rerank`和`Cohere Rerank`。

重排序Rerank主要用于初步检索结果的排序，提高最终输出的相关性和准确性。

比如先从100万chunk中召回100个chunk，然后通过Rerank模型让提问和这100个chunk进行语义重排序，取出重排序后的Top5再对LLM进行提问。

**CASE1：BGE-Rerank使用：**

先下载BGE-Rerank模型：

```python
from modelscope import snapshot_download
model_dir = snapshot_download('BAAI/bge-reranker-large', cache_dir='/root/autodl-tmp/models')
```

再使用BGE-Rerank：

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')
model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')
model.eval()

pairs = [['what is panda?', 'The giant panda is a bear species endemic to China.']]
inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')
scores = model(**inputs).logits.view(-1).float()
print(scores)
```

执行结果：

```shell
4.9538
# 在BGE-Rerank模型中，相关性分数scores是一个未归一化的对数几率（logits）值，范围没有固定的上
# 限或下限（不像某些模型限制在0-1）。不过BGE-Rerank的分数通常落在以下范围：
# 高相关性： 3.0~10.0
# 中等相关性：0.0~3.0
# 低相关性/不相关：负数（如-5.0以下）

# 可以看出两者的相关性属于高相关性
```

##### 2.2、混合检索

结合向量检索和关键词检索的优势，通过重排序模型对结果进行归一化处理，提升召回质量。

### 3、优化查询拓展

##### 3.1、相似语义改写

使用大模型将用户的query改写成多个语义相近的查询，从而提升召回多样性。

##### 3.2、双向改写

将查询改写成文档（Query2Doc）或为文档生成查询（Doc2Query），缓解短文本向量化效果差的问题。

**Query2Doc：** 通过先对Query进行回答，然后通过预先的回答去chunks中召回

**Doc2Query：** 对Doc预先设定有可能出现的Query，从而提高召回效果。

原理就是增加更多可能的抓手，提高召回正确率

### 4、索引拓展

##### 4.1、离散索引拓展

使用关键词抽取、实体识别等技术生成离散索引，与向量检索互补，提升召回准确性。

比如一段chunk如下：

```css
本文介绍了深度学习模型训练中的优化技巧，包括：
1. 使用 AdamW 优化器替代传统的 SGD。
2. 采用混合精度训练，减少显存占用。
3. 使用分布式训练技术加速大规模模型的训练。
```

通过关键词提取技术可以提取如下的关键词：

```css
["深度学习", "模型训练", "优化技巧", "AdamW", "混合精度训练", "分布式训练"]
```

当用户查询“如何优化深度学习模型训练？”时，离散索引中的关键词能够快速匹配到相关文档。

##### 4.2、连续索引扩展

结合多种向量模型（如OpenAI的Ada、智源的BGE）进行多路召回，取长补短。

##### 4.3、混合索引召回

将BM25等离散索引与向量索引结合，通过Ensemble Retriever实现混合召回，提升召回多样性。

### 5、Small-to-Big索引策略

Small-to-Big是一种高效的检索方法，特别适用于处理长文档或多文档场景。核心思想是通过小规模内容（如摘要、关键句或段落）建立索引，并链接到大规模内容主体中。这种策略的优势在于能够快速定位相关的小规模内容，并通过链接获取更详细的上下文信息，从而提高检索效率和答案的逻辑连贯性。

比如一篇很长的文档，可以对其进行提取摘要，当用户输入Query后，先在摘要中检索，摘要又映射到长doc，召回长文档作为上下文对LLM进行提问。
