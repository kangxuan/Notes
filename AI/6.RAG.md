# RAG

### æ¦‚å¿µ

Retrieval Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ã€‚RAGé€šè¿‡å®æ—¶æ£€ç´¢ç›¸å…³æ–‡æ¡£æˆ–ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œæé«˜ç”Ÿæˆç»“æœçš„æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚

![ragæµç¨‹å›¾](/Users/kx/workspace/Notes/AI/images/ragæµç¨‹.png)

### RAGçš„æ ¸å¿ƒç”¨é€”

**1ã€è§£å†³çŸ¥è¯†æ—¶æ•ˆæ€§é—®é¢˜**

LLMçš„è®­ç»ƒæ•°æ®é€šå¸¸æ˜¯é™æ€çš„ï¼Œæ— æ³•æ¶µç›–æœ€æ–°çš„ä¿¡æ¯ï¼Œè€ŒRAGå¯ä»¥æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“å®æ—¶æ›´æ–°ä¿¡æ¯ã€‚

**2ã€è§£å†³å¹»è§‰é—®é¢˜**

é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†åº“ï¼ŒRAGèƒ½å¤Ÿå‡å°‘æ¨¡å‹ç”Ÿæˆè™šå‡æˆ–ä¸å‡†ç¡®çš„èƒ¡ç¼–ä¹±é€ ã€‚

**3ã€æå‡ä¸“ä¸šé¢†åŸŸå›ç­”è´¨é‡**

RAGèƒ½å¤Ÿç»“åˆå‚ç›´é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†åº“ï¼Œç”Ÿæˆæ›´ä¸“ä¸šæ›´æœ‰æ·±åº¦çš„å›ç­”ã€‚

### RAGçš„æ ¸å¿ƒåŸç†å’Œæµç¨‹

è¿™é‡Œè¯´çš„æ˜¯NativeRAGï¼Œå³æ¨ç†ä¸­ä½¿ç”¨RAGã€‚

**1ã€æ•°æ®é¢„å¤„ç†**

- çŸ¥è¯†åº“æ„å»ºï¼šæ”¶é›†å¹¶æ•´ç†æ–‡æ¡£ã€ç½‘é¡µã€æ•°æ®åº“ç­‰å¤šæºæ•°æ®ï¼Œæ„å»ºçŸ¥è¯†åº“ï¼Œä¸€èˆ¬ç”±æ•°æ®æ²»ç†å›¢é˜Ÿå»åšã€‚

- æ–‡æ¡£åˆ†å—ï¼šå°†æ–‡æ¡£åˆ‡åˆ†ä¸ºåˆé€‚å¤§å°çš„å—ï¼ˆchunksï¼‰ï¼Œä»¥ä¾¿åç»­æ£€ç´¢ã€‚

- å‘é‡åŒ–å­˜å‚¨ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼ˆå¦‚BGEã€M3Eç­‰Embeddingæ¨¡å‹ï¼‰å°†chunkæ¢æ¢ä¸ºç»Ÿä¸€ç»´åº¦çš„å‘é‡ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­

**2ã€æ£€ç´¢é˜¶æ®µ**

- æŸ¥è¯¢å¤„ç†ï¼šå°†ç”¨æˆ·çš„é—®é¢˜ï¼ˆqueryï¼‰è½¬æ¢æˆå‘é‡ï¼Œå¹¶åœ¨å‘é‡æ•°æ®åº“ä¸­è¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢ï¼Œå¹¶è¿”å›æœ€ç›¸ä¼¼çš„ï¼ˆæœ€ç›¸å…³çš„ï¼‰chunkã€‚

- é‡æ’åºï¼šå¯¹æ£€ç´¢è¿”å›çš„æ•°æ®è¿›è¡Œç›¸å…³æ€§æ’åºï¼Œé€‰æ‹©æœ€ç›¸å…³çš„ç‰‡æ®µä½œä¸ºç”Ÿæˆé˜¶æ®µçš„è¾“å…¥å’Œä¸Šä¸‹æ–‡ã€‚

**3ã€ç”Ÿæˆé˜¶æ®µ**

- ä¸Šä¸‹æ–‡ç»„è£…ï¼šå°†ç”¨æˆ·çš„é—®é¢˜ï¼ˆqueryï¼‰å’Œæ£€ç´¢åˆ°çš„chunkç»“åˆï¼Œå½¢æˆå¢å¼ºçš„ä¸Šä¸‹æ–‡è¾“å…¥

- ç”Ÿæˆå›ç­”ï¼šLLMåŸºäºå¢å¼ºçš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”ã€‚

![RAGæ­¥éª¤](/Users/kx/workspace/Notes/AI/images/ragæ­¥éª¤.png)

### åµŒå…¥æ¨¡å‹ï¼ˆEmbeddingæ¨¡å‹ï¼‰é€‰æ‹©å’Œåˆ†ç±»

Embeddingæ¨¡å‹æ’ååœ¨huggingfaceç½‘ç«™ä¸­å®æ—¶æ›´æ–°ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©ï¼š[Embeddingæ’åç½‘ç«™]([MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard))

**1ã€é€šç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹**

- BGE-M3ï¼ˆæ™ºæºç ”ç©¶é™¢ï¼‰ï¼šæ”¯æŒ100+è¯­è¨€ï¼Œèåˆå¯†é›†ã€ç¨€ç–ã€å¤šå‘é‡æ··åˆæ£€ç´¢ï¼Œé€‚åˆè·¨è¯­è¨€é•¿æ–‡æ¡£æ£€ç´¢ã€‚é€‚åˆè·¨è¯­è¨€é•¿æ–‡æ¡£æ£€ç´¢ã€é«˜ç²¾åº¦RAGåº”ç”¨ã€‚

- text-embedding-3-largeï¼ˆOpenAIï¼‰ï¼šå‘é‡ç»´åº¦3072ï¼Œé•¿æ–‡æœ¬è¯­ä¹‰æ•æ‰èƒ½åŠ›å¼ºï¼Œè‹±æ–‡è¡¨ç°ä¼˜ç§€ã€‚è‹±æ–‡å†…å®¹ä¼˜å…ˆçš„å…¨çƒåŒ–åº”ç”¨ã€‚

**2ã€ä¸­æ–‡åµŒå…¥æ¨¡å‹**

**3ã€æŒ‡ä»¤é©±åŠ¨å’Œå¤æ‚ä»»åŠ¡æ¨¡å‹**

**4ã€ä¼ä¸šçº§ä¸å¤æ‚ç³»ç»Ÿ**

### CASE1ï¼šBGE-M3ä½¿ç”¨

é’ˆå¯¹äºBGE-M3æ¨¡å‹çš„ä½¿ç”¨ï¼Œéœ€è¦è¿è¡Œåœ¨GPUä¸Šï¼Œå¯ä»¥åœ¨AutoDLä¸Šç§Ÿç”¨GPUæœåŠ¡å™¨è¯•ç”¨ã€‚

**1ã€å…ˆä¸‹è½½å¿…è¦çš„åŒ…**

```shell
pip install modelscope
pip install FlagEmbedding
```

**2ã€ä¸‹è½½bge-m3æ¨¡å‹** 

```python
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('BAAI/bge-m3', cache_dir='/root/autodl-tmp/models')
```

**3ã€ä½¿ç”¨bge-m3è®¡ç®—ç›¸ä¼¼åº¦**

```python
from FlagEmbedding import BGEM3FlagModel

# å£°æ˜Embeddingæ¨¡å‹
model = BGEM3FlagModel('/root/autodl-tmp/models/BAAI/bge-m3',  
                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation
#
sentences_1 = ["What is BGE M3?", "Defination of BM25"]
sentences_2 = ["BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.", 
               "BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document"]

# å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
embeddings_1 = model.encode(sentences_1, 
                            batch_size=12, 
                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.
                            )['dense_vecs']
# å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡
embeddings_2 = model.encode(sentences_2)['dense_vecs']
# è®¡ç®—ä¸¤ç»„Embeddingçš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œ@ ç¬¦å·åœ¨Pythonä¸­è¡¨ç¤ºçŸ©é˜µä¹˜æ³•è¿ç®—
similarity = embeddings_1 @ embeddings_2.T
print(similarity)
```

è¿è¡Œç»“æœï¼š

```shell
[[0.626 0.3477]
[0.3499 0.678 ]]
```

ä»è¾“å…¥ç»“æœå¯ä»¥çœ‹å‡ºsentences_1ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ å’Œsentences_2ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ›´ç›¸ä¼¼ï¼Œsentences_1ä¸­çš„ç¬¬äºŒä¸ªå…ƒç´ å’Œsentences_2ä¸­çš„ç¬¬äºŒä¸ªå…ƒç´ æ›´ç›¸ä¼¼ã€‚

### CASE2ï¼šgte-qwen2ä½¿ç”¨

**1ã€ä¸‹è½½å¿…è¦çš„åŒ…**

```shell
pip install modelscope
pip install FlagEmbedding
```

**2ã€ä¸‹è½½gte-qwen2æ¨¡å‹**

get-qwen2-7Bæ¨¡å‹å¤ªå¤§ï¼Œè¿™é‡Œæµ‹è¯•ä½¿ç”¨1.5B

```python
from modelscope import snapshot_download
#model_dir = snapshot_download('iic/gte_Qwen2-7B-instruct', cache_dir='/root/autodl-tmp/models')
model_dir = snapshot_download('iic/gte_Qwen2-1.5B-instruct', cache_dir='/root/autodl-tmp/models')
```

**3ã€ä½¿ç”¨gte-qwen2è®¡ç®—ç›¸ä¼¼åº¦**

```python
from sentence_transformers import SentenceTransformer

model_dir = "/root/autodl-tmp/models/iic/gte_Qwen2-1___5B-instruct"
model = SentenceTransformer(model_dir, trust_remote_code=True)
# è®¾ç½®å‘é‡ç»´åº¦
model.max_seq_length = 8192

queries = [
    "how much protein should a female eat",
    "summit define",
]
documents = [
    "As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",
    "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.",
]

# å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡
query_embeddings = model.encode(queries, prompt_name="query")
document_embeddings = model.encode(documents)

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
scores = (query_embeddings @ document_embeddings.T) * 100
print(scores.tolist())
```

è¿è¡Œç»“æœï¼š

```shell
[[70.00668334960938, 8.184843063354492], [14.62419319152832, 77.71407318115234]]
```

å’ŒBGE-M3æ•ˆæœå·®ä¸å¤šï¼Œæ€»ä½“æ˜¯å¯ä»¥çœ‹å‡ºè°å’Œè°æ›´ç›¸ä¼¼ã€‚

### CASE3ï¼šDeepseek+Faissæ­å»ºæœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢

##### éœ€æ±‚

æä¾›äº†ä¸€ä»½åä¸º`æµ¦å‘ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œè¥¿å®‰åˆ†è¡Œä¸ªé‡‘å®¢æˆ·ç»ç†è€ƒæ ¸åŠæ³•.pdf`çš„æ–‡ä»¶ï¼Œè¦æ±‚é€šè¿‡æ­å»ºæœ¬åœ°çŸ¥è¯†åº“ï¼Œç»“åˆLLMå®ç°å†…éƒ¨çŸ¥è¯†ç‚¹é—®ç­”ã€‚

##### å®ç°æµç¨‹ï¼š

**1ã€ä»PDFä¸­æå–æ–‡æœ¬**

å°†æä¾›çš„PDFæ–‡æ¡£é€šè¿‡`PdfReader`å·¥å…·è¯»å–æ–‡æœ¬

**2ã€åˆ†å—å­˜å…¥å‘é‡æ•°æ®åº“ä¸­**

å°†æ–‡æœ¬æŒ‰ç…§chunk_size=800,chunk_overlap=150ï¼ˆoverlapæ˜¯å‰åçš„å†—ä½™é‡ï¼‰è¿›è¡Œåˆ‡åˆ†ï¼Œç„¶åå°†åˆ‡åˆ†å¥½çš„chunké€šè¿‡`text-embedding-v1`åµŒå…¥æ¨¡å‹è½¬æ¢æˆåŒç»´åº¦çš„å‘é‡ï¼Œç„¶åå°†chunkå’Œè½¬æ¢åçš„å‘é‡å­˜å‚¨åˆ°Faissæœ¬åœ°å‘é‡æ•°æ®åº“ä¸­

**3ã€ç”Ÿæˆå›ç­”**

ç”¨æˆ·æé—®ï¼ˆqueryï¼‰ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢åˆ°ç›¸ä¼¼çš„chunkï¼Œç»„åˆæˆå¢å¼ºçš„promptå‘LLMæé—®ï¼Œè¿”å›å¢å¼ºçš„å›ç­”ã€‚

```python
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
import os, pickle
from typing import List

DASHSCOPE_API_KEY = "sk-xxxxxxx"

# ======================
# ğŸ§© PDF æ–‡æœ¬æå– + é¡µç 
# ======================
def extract_text_with_page_numbers(pdf) -> (str, List[int]):
    text = ""
    page_numbers = []

    for page_number, page in enumerate(pdf.pages, start=1):
        extracted = page.extract_text()
        if extracted:
            lines = extracted.split("\n")
            text += extracted + "\n"
            page_numbers.extend([page_number] * len(lines))
        else:
            print(f"âš ï¸ Page {page_number} æ— æ–‡æœ¬")

    return text, page_numbers


# ======================
# ğŸ§© å‘é‡å­˜å‚¨
# ======================
def build_vector_db(text: str, page_numbers: List[int], save_path="./vector_db"):

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
    )
    chunks = splitter.split_text(text)
    print(f"æ–‡æœ¬åˆ†æˆ {len(chunks)} å—")

    embeddings = DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=DASHSCOPE_API_KEY
    )

    db = FAISS.from_texts(chunks, embeddings)

    # å»ºç«‹æ–‡æœ¬â†’é¡µç æ˜ å°„
    db.page_info = {chunk: page_numbers[i] for i, chunk in enumerate(chunks)}

    os.makedirs(save_path, exist_ok=True)
    db.save_local(save_path)

    with open(os.path.join(save_path, "page_info.pkl"), "wb") as f:
        pickle.dump(db.page_info, f)

    print("å‘é‡åº“å­˜å‚¨å®Œæˆ")
    return db

# åŠ è½½å‘é‡åº“
def load_vector_db(path="./vector_db"):
    embeddings = DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=DASHSCOPE_API_KEY
    )
    db = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

    with open(os.path.join(path, "page_info.pkl"), "rb") as f:
        db.page_info = pickle.load(f)

    print("å‘é‡åº“åŠ è½½æˆåŠŸ")
    return db


# ======================
# ğŸ§© LLM + æ£€ç´¢ + QA ç»„åˆé“¾ï¼ˆLCELæ ¸å¿ƒï¼‰
# ======================
llm = Tongyi(model_name="deepseek-v3", dashscope_api_key=DASHSCOPE_API_KEY)

prompt = PromptTemplate.from_template(
    """ä½¿ç”¨ä»¥ä¸‹å†…å®¹å›ç­”æœ€ç»ˆé—®é¢˜ï¼š
å†…å®¹ï¼š{context}
é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š"""
)

def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

def ask(knowledgeBase, query):
    # ä»å‘é‡æ•°æ®åº“ä¸­å¬å›å¤šå°‘ä¸ªchunkï¼ˆk=5ï¼Œæ˜¯å¬å›5ä¸ªchunkï¼‰
    retriever = knowledgeBase.as_retriever(k=5)

    chain = (
        RunnableParallel({
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
        })
        | prompt
        | llm
    )

    result = chain.invoke(query)
    print("\nğŸ§  å›ç­”ï¼š\n", result)

    print("\nğŸ“Œ æ¥æºé¡µç ï¼š")
    docs = knowledgeBase.similarity_search(query, k=5)
    pages = set()
    for d in docs:
        page = knowledgeBase.page_info.get(d.page_content, "æœªçŸ¥")
        if page not in pages:
            pages.add(page)
            print(f"- ç¬¬ {page} é¡µ")


# ======================
# ğŸš€ ä¸»æµç¨‹
# ======================
pdf_reader = PdfReader("./æµ¦å‘ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œè¥¿å®‰åˆ†è¡Œä¸ªé‡‘å®¢æˆ·ç»ç†è€ƒæ ¸åŠæ³•.pdf")

text, page_numbers = extract_text_with_page_numbers(pdf_reader)

knowledgeBase = build_vector_db(text, page_numbers)

ask(knowledgeBase, "å®¢æˆ·ç»ç†æ¯å¹´è¯„è˜ç”³æŠ¥æ—¶é—´æ˜¯æ€æ ·çš„ï¼Ÿ")
```

è¿è¡Œç»“æœï¼š

```shell
æ–‡æœ¬åˆ†æˆ 6 å—
å‘é‡åº“å­˜å‚¨å®Œæˆ

ğŸ§  å›ç­”ï¼š
 å®¢æˆ·ç»ç†æ¯å¹´è¯„è˜ç”³æŠ¥æ—¶é—´æ˜¯**æ¯å¹´ä¸€æœˆä»½**ã€‚

ä¾æ®æ–‡ä»¶ç¬¬å…­ç« ç¬¬åä¸€æ¡è§„å®šï¼šâ€œæ¯å¹´ä¸€æœˆä»½ä¸ºå®¢æˆ·ç»ç†è¯„è˜çš„ç”³æŠ¥æ—¶é—´ï¼Œç”±åˆ†è¡ŒäººåŠ›èµ„æºéƒ¨ã€ä¸ªäººä¸šåŠ¡éƒ¨æ¯å¹´äºŒæœˆä»½ç»„ç»‡ç»Ÿä¸€çš„èµ„æ ¼è€ƒè¯•ã€‚â€

ğŸ“Œ æ¥æºé¡µç ï¼š
- ç¬¬ 1 é¡µ
```

### 
