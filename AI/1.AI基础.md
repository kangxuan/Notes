# 基础概念

#### 什么是AI

AI的核心目的是让机器能够执行需要人类智能才能完成的任务。

#### AI的发展阶段

**1、早期阶段**

以规则为主的专家系统，根据预设的逻辑和规则。预设的规则是非常局限的，是不靠谱的。

**2、机器学习**

通过数据训练模型，使机器从数据中寻找规律。训练的数据有限，也是不靠谱的。

**3、深度学习**

利用神经网络模拟人类大脑的复杂结构，处理更复杂的任务。训练的参数相比大模型还是太少，不够智能。

**4、大模型**

以大规模数据和算力为基础，构建通用性强、卓越性能的AI模型。随着训练的参数增加，会慢慢接近人脑，人脑大约为250万亿。

#### AI的分类

**1、分析式AI**

- 又叫预判式AI，其核心是通过对已有数据进行分析、分类、预测和决策。

- 优势在于其高精度、高效性；劣势在于只能处理已有数据的模式，创造性不足。

**2、生成式AI**

- 专注于创造新内容，比如：文本生成、图像生成、视频生成等。

- 优势在于其创造性和灵活性；但也面临着数据安全、版权等问题。

#### 大语言模型

大语言模型是一种通用自然语言生成模型，使用大量语料来训练，以实现语言生成、对话生成、问题回答等。

其中ChatGPT3.5使用了4.5TB纯文本语料来训练的。

**1、基础能力**

- 语言生成

- 上下文学习

- 世界知识

**2、超能力**

- 响应人类指令

- 泛化到没有见到的任务，比如说：让AI用鲁迅的口吻写一篇足浴的宣传广告

- 代码生成和理解

#### GPT的演变过程

**1、GPT1**

**时间**：2018

**训练参数**：1.17亿

**亮点**：有一定泛化能力，能用于和监督任务无关的NLP任务

**2、GPT2**

时间：2019

训练参数：15亿

亮点：除了理解能力外，GPT-2 在生成方面表现出了强大的天赋：阅读摘要、聊天、编故事，生成假新闻、钓鱼邮件、在线角色扮演等

**3、GPT3**

时间：2020

训练参数：1750亿

亮点：自监督模型，可以完成自然语言处理的绝大部分任务：写代码、创作、写剧本，模仿某个人的文字等。

**4、InstructGPT**

时间：2022.01

训练参数：无

亮点：一个经过微调的新版 GPT-3，可将有害、不真实的、有偏差的输出最小化

5、ChatGPT

时间：2022.12

训练参数：无

亮点：InstructGPT的衍生品，它将人类的反馈纳入训练过程，更好地让模型与用户意图保持一致。

#### ChatGPT是怎么训练出来的

**步骤一：收集数据，微调监督模型**

从问题库中抽取问题，专家进行监督并给出答案，并基于GPT3.5做微调得到监督模型。这一阶段就是在给机器喂数据。

**步骤二：收集比较数据，训练奖励模型**

让AI针对一个问题给出多个回答，通过人工进行好坏排序，从而训练出奖励模型。最初人工排序都是在非洲请的人工进行标注（RankList标注平台），因为非洲的人工成本很廉价。

**步骤三：收集数据，强化学习优化模型**

让AI针对一个问题给出多个回答，通过奖励模型来进行排序，并持续不断的迭代模型。这一阶段就不需要人工来标记，效率提升非常大，所以后续升级迭代非常快速。

#### AI在企业中的应用

在企业中，最重要的是解决垂直领域的问题

**1、私有化数据**

**2、小模型**

针对小模型，进行微调+蒸馏，使其在垂直领域接近大模型甚至超过大模型。

#### 通用模型和推理模型的选择

通用模型：日常编程、快速开发、前端代码生成、常规脚本任务。

推理模型：数学密集型计算、复杂算法、代码逻辑深度优化、需要推理过程的任务。

# 专业术语

#### prompt

提示词工程

#### Function Call

基于LLM大语言模型的理解能力，通过理解语义，自主决定调用某些工具，并结构化调用。通俗地讲function call是让模型“通过结构化方式调用外部函数或工具”的机制，目的是让它变得更实用、更像一个能“干活”的助手，而不只是生成文字。

举例：

```
提问：我明天应该穿什么衣服？

理解语义，自主决定使用天气查询工具
结构化调用，需要{"region":?,"date":"2025-05-16"}，联网查询到明天的天气，然后填槽
补全region，再次询问
基于返回的结构化数据，重新整合再输出
```

大模型本质是一个离线的model，它的知识推理是通过大脑来完成的，如果要获取时间、天气，需要第三方的服务，即Function Call。

#### RAG

背景知识，也就是常说的知识库

#### Fine Tuning

微调，针对某个垂直领域，给模型更多的数据，进行训练。

#### Agent

智能体

#### ChatBI（问数）

询问基于数据的问题。

#### MCP

#### 蒸馏

蒸馏类似老师教学生，将一个大尺寸模型的能力迁移到小尺寸模型上，比如671B=>7B。

尺寸大的模型能力强，用作老师，让老师整理一些训练出来（Q、A、Reasoning），让小模型进行微调 => 相比之前有所提升，甚至在某个垂直领域，接近大模型甚至超过大模型。

# 私有化部署

#### Vllm使用

Vllm使用：是由伯克利大学 LMSYS 组织开源的LLM高速推理框架，用于提升LLM的吞吐量与内存使用效率。它通过 PagedAttention 技术高效管理注意力键和值的内存，并结合连续批处理技术优化推理性能。vLLM 支持量化技术、分布式推理、与 Hugging Face 模型无缝集成等功能。

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

- vllm serve，启动 vLLM 推理服务的命令

- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B，Hugging Face 模型库中的模型名称，vLLM 会尝试从 HF 下载模型

- --tensor-parallel-size 2，启用张量并行，在 2 个 GPU 上分布式运行模型（适合 32B 大模型）

- --max-model-len 32768，设置模型的最大上下文长度（32K tokens），确保能处理长文本。

- --enforce-eager，禁用 CUDA Graph 优化（可能在某些环境下更稳定，但性能稍低）

#### 云GPU部署（以AutoDL为例）

**1、注册AutoDL账号**

在https://autodl.com上注册

**2、租用GPU服务器**

在https://autodl.com/market/list页面选择合适的GPU，如果是学习用可选择4090按量计费，充值，并支付开机。

**3、开启服务器**

在`容器实例`列表中找到租用的服务器，点击快捷工具-ipynb工具，进入。

**4、下载模型**

进入实例后，新建py文件，并输入：

```shell
from modelscope import snapshot_download
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir="/root/autodl-tmp/models")

# 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'：是魔搭社区的模型下载地址
# cache_dir：是将大模型下载到那个文件夹下，AutoDL推荐下载到audodl-tmp下
```

如果提示modelscope未下载，可打开终端执行安装modelscope或者相关依赖：

```shell
pip install modelscope
```

**5、调用本地大模型**

在`/root/autodl-tmp`下创建文件夹`code`,并新建py文件：

```py
from modelscope import AutoModelForCausalLM, AutoTokenizer
model_name = "/root/autodl-tmp/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="cuda" # auto
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
prompt = "帮我写一个二分查找法"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2000
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

#### Ollama使用

这里以Mac为例

**1、下载Ollama客户端**

去官网下载：[Download Ollama on macOS](https://ollama.com/download/mac)

**2、下载模型**

```shell
ollama pull deepseek-r1:1.5b
```

**3、运行模型**

```shell
ollama run deepseek-r1:1.5b
```

# API调用

这里以阿里云百炼为例，阿里的通义千问系列在国内算是不错的。使用前需要前往阿里云百炼平台开通并购买对应的模型并获得AK。

以下例子都使用python举例，不同语言具体参考文档：[大模型服务平台百炼控制台](https://bailian.console.aliyun.com/?spm=5176.12818093_47.console-base_product-drawer-right.dsfm.10b22cc9ZCWkAa&tab=doc#/doc)

百炼提供了两种python包来调用AI，一种是OpenAI Python SDK，另一种是DashScope Python SDK。以下举例使用DashScope。

```shell
pip install dashscope
```

#### 简单调用

新建py文件：

```python
import json
import os
import dashscope
from dashscope.api_entities.dashscope_response import Role
dashscope.api_key = "xxx" # 你的AK
# 封装模型响应函数
def get_response(messages):
    response = dashscope.Generation.call(
        model='qwen-plus', # 选择的模型
        messages=messages, # 输入的消息
        result_format='message' # 将输出设置为message形式
    )
    return response

if __name__ == "__main__":
    review = '真的垃圾'
    messages = [
        {"role": "system", "content": "你是一名舆情分析师，帮我判断产品口碑的正负向，回复请用一个词语：正向 或者 负向"},
        {"role": "user", "content": review}
    ]
    response = get_response(messages)
    print(response.output.choices[0].message.content)
```

运行py文件：

```shell
负向
```
